#!/bin/bash
#
# Copyright 2016 (c) BlueData Software, Inc.
#
#

set -o pipefail
SELF=$(readlink -nf $0)
export CONFIG_BASE_DIR=$(dirname ${SELF})

source ${CONFIG_BASE_DIR}/logging.sh
source ${CONFIG_BASE_DIR}/utils.sh

if [[ "$1" == "--addnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--delnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--configure" ]]; then
    log "Starting configuration ... "

    ## Fall through to start the configuration.
else
    echo "ERROR: Unknown command line option(s): '$@'"
    exit 10
fi

source ${CONFIG_BASE_DIR}/macros.sh

####################### AUTOGENERATED CODE STARTS BELOW #######################
#
[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/rstudioserver /etc/init.d/rstudioserver

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/spark-master /etc/init.d/spark-master

[ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/spark-defaults.conf /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/start_jupyter.sh /etc/init.d/start_jupyter.sh

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/shinyserver /etc/init.d/shinyserver

[ ! -d '/usr/bin/' ] && mkdir -vp /usr/bin/
cp -rvf ${CONFIG_BASE_DIR}/pam_mkhomedir.sh /usr/bin/pam_mkhomedir.sh

[ ! -d '/usr/lib/rstudio-server/conf/' ] && mkdir -vp /usr/lib/rstudio-server/conf/
cp -rvf ${CONFIG_BASE_DIR}/start_rstudioserver.sh /usr/lib/rstudio-server/conf/start_rstudioserver.sh

[ ! -d '/usr/bin' ] && mkdir -vp /usr/bin
cp -rvf ${CONFIG_BASE_DIR}/hadoop /usr/bin/hadoop

[ ! -d '/etc/jupyterhub' ] && mkdir -vp /etc/jupyterhub
cp -rvf ${CONFIG_BASE_DIR}/jupyterhub_config.py /etc/jupyterhub/jupyterhub_config.py

[ ! -d '/usr/local/share/jupyter/kernels/apache_toree_sql' ] && mkdir -vp /usr/local/share/jupyter/kernels/apache_toree_sql
cp -rvf ${CONFIG_BASE_DIR}/sq_kernel.json /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/start_jupyterhub.sh /etc/init.d/start_jupyterhub.sh

[ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/core-site.xml /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/core-site.xml

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/jupyterhub /etc/init.d/jupyterhub

[ ! -d '/root/.jupyter/' ] && mkdir -vp /root/.jupyter/
cp -rvf ${CONFIG_BASE_DIR}/jupyter_notebook_config.py /root/.jupyter/jupyter_notebook_config.py

[ ! -d '/opt/bluedata/vagent/guestconfig/appconfig' ] && mkdir -vp /opt/bluedata/vagent/guestconfig/appconfig
cp -rvf ${CONFIG_BASE_DIR}/appjob /opt/bluedata/vagent/guestconfig/appconfig/appjob

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/spark-slave /etc/init.d/spark-slave

[ ! -d '/usr/local/share/jupyter/kernels/apache_toree_pyspark' ] && mkdir -vp /usr/local/share/jupyter/kernels/apache_toree_pyspark
cp -rvf ${CONFIG_BASE_DIR}/p_kernel.json /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json

[ ! -d '/etc/init.d/' ] && mkdir -vp /etc/init.d/
cp -rvf ${CONFIG_BASE_DIR}/jupyter-server /etc/init.d/jupyter-server

[ ! -d '/etc/sudoers.d' ] && mkdir -vp /etc/sudoers.d
cp -rvf ${CONFIG_BASE_DIR}/jupyter /etc/sudoers.d/

[ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
cp -rvf ${CONFIG_BASE_DIR}/spark-env.sh /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh

[ ! -d '/etc/pam.d/' ] && mkdir -vp /etc/pam.d/
cp -rvf ${CONFIG_BASE_DIR}/rstudio /etc/pam.d/rstudio

[ ! -d '/opt/shiny-server/conf/' ] && mkdir -vp /opt/shiny-server/conf/
cp -rvf ${CONFIG_BASE_DIR}/start_shinyserver.sh /opt/shiny-server/conf/start_shinyserver.sh

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@IP@@@@ /root/.jupyter/jupyter_notebook_config.py GET_NODE_FQDN

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@SPARK_MAX_CORES@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf GET_TOTAL_VCORES

REPLACE_PATTERN @@@@MASTER_HOST@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh GET_FQDN_LIST controller

REPLACE_PATTERN @@@@MEMORY@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh echo $(GET_TOTAL_VMEMORY_MB)m

REPLACE_PATTERN @@@@CORES@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh GET_TOTAL_VCORES

REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-slave GET_NODE_FQDN

REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-slave echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /etc/init.d/spark-slave GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-master GET_FQDN_LIST controller

REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-master echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7

REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-master echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7

REPLACE_PATTERN @@@@AWS_ACCESS_KEY@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/core-site.xml TENANT_INFO s3_access_key

REPLACE_PATTERN @@@@AWS_SECRET_KEY@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/core-site.xml TENANT_INFO s3_secret_key


if [[ "${NODE_ROLE}" == 'controller' ]] ||\
 [[ "${NODE_ROLE}" == 'worker' ]]; then 
    bash ${CONFIG_BASE_DIR}/total_vcores.sh || exit 2
fi

REGISTER_START_SERVICE_SYSCTL rstudio-server rstudioserver
REGISTER_START_SERVICE_SYSCTL jupyterhub jupyterhub
REGISTER_START_SERVICE_SYSCTL shiny-server shinyserver
REGISTER_START_SERVICE_SYSCTL spark_worker spark-slave
REGISTER_START_SERVICE_SYSCTL spark_master spark-master
REGISTER_START_SERVICE_SYSCTL jupyter-notebook jupyter-server
