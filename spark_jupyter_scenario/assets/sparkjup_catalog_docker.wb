#!/usr/bin/env bdwb
################################################################################
#                                                                              #
#  Sample workbench instructions for building a BlueData catalog entry.        #
#                                                                              #
################################################################################
#
# YOUR_ORGANIZATION_NAME must be replaced with a valid organization name. Please
# refer to 'help builder organization' for details.
#
builder organization --name BlueData

## Begin a new catalog entry
catalog new --distroid spark210NB --name "Spark 2.1.0 with Notebooks and Security"                           \
            --desc "Spark 2.1.0 with Notebooks and security"					\
            --categories Spark --version 4.0

## Define all node roles for the virtual cluster.
role add controller 1
role add worker 0+

## Define all services that are available in the virtual cluster.
service add --srvcid jupyter-notebook --name "Jupyter Notebook" --scheme "http" --port 8888   \
            --path "/" --display \
	    --sysv jupyter-server

service add --srvcid spark --name "Spark master" --scheme "http" --port 8080   \
            --path "/" --display


service add --srvcid spark_worker --name "Spark worker" --scheme "http"        \
            --port 8081 --path "/" --display \
	    --sysv spark-slave
service add --srvcid spark_master --name "Spark master" --scheme "spark"       \
            --port 7077 --export_as "spark" \
	    --sysv spark-master
service add --srvcid zeppelin-notebook --name "Zeppelin server" --scheme "http" \
            --port 8055 --path "/" --display \
	    --sysv zeppelin-server
service add --srvcid mysql --name "Mysql server" --port 3306 \
	    --sysv mysqld

## Define run time placement of the services
clusterconfig new --configid default
clusterconfig assign --configid default --role controller                  \
                     --srvcids mysql spark spark_master spark_worker zeppelin-notebook jupyter-notebook

clusterconfig assign --configid default --role worker --srvcids spark_worker 

## Appconfiguration autogenenration.
appconfig autogen --new --configapi 4
 
appconfig autogen --pkgfile jupyter_notebook_config.py    	 \
		  --destdir /root/.jupyter/
appconfig autogen --pkgfile spark-defaults.conf spark-env.sh   \
                  --destdir /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf
appconfig autogen --pkgfile core-site.xml --dest /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml
appconfig autogen --pkgfile hadoop --dest /usr/bin/hadoop
appconfig autogen --pkgfile spark-master spark-slave jupyter-server start_jupyter.sh zeppelin-server 	 \
                  --destdir /etc/init.d/
appconfig autogen --pkgfile zeppelin-env.sh zeppelin-site.xml zeppelin-log4j.properties shiro.ini \
                  --destdir /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/

appconfig autogen --pkgfile kernel.json --dest /usr/local/share/jupyter/kernels/apache_toree_pyspark/

#Replace
appconfig autogen --replace /etc/init.d/zeppelin-server --pattern @@@@ZEPPELIN_HOME@@@@               \
                  --macro "echo /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT"
		  
## TODO : Need to convert below shell script to proper macro for replacing zeppelin_cores in
## the file zeppelin-env.sh
appconfig autogen --execute total_vcores.sh		  

#appconfig autogen --replace /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh \
                  --pattern @@@@ZEPPELIN_CORES@@@@ --macro GET_TOTAL_VCORES

appconfig autogen --replace /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh  \
                  --pattern @@@@SPARK_MASTER@@@@ --macro GET_SERVICE_URL spark_master controller

appconfig autogen --replace /usr/lib/zeppelin/zeppelin-0.8.0-SNAPSHOT/conf/zeppelin-env.sh  \
                  --pattern @@@SPARK_HOME@@@ --macro echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7

appconfig autogen --replace /root/.jupyter/jupyter_notebook_config.py	 \
                  --pattern @@@@IP@@@@ --macro GET_FQDN_LIST controller


appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-defaults.conf \
                  --pattern @@@@SPARK_MASTER@@@@ --macro GET_SERVICE_URL spark_master controller
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@MASTER_HOST@@@@ --macro GET_FQDN_LIST controller
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@MEMORY@@@@ --macro echo "$(GET_TOTAL_VMEMORY_MB)m"
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@CORES@@@@ --macro GET_TOTAL_VCORES
appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@FQDN@@@@ --macro GET_NODE_FQDN
appconfig autogen --replace /etc/init.d/spark-master --pattern @@@@FQDN@@@@                   \
                  --macro GET_FQDN_LIST controller


appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@SPARK_HOME@@@@              \
                  --macro "echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7"
appconfig autogen --replace /etc/init.d/spark-master --pattern @@@@SPARK_HOME@@@@             \
                  --macro "echo /usr/lib/spark/spark-2.1.0-bin-hadoop2.7"
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml --pattern @@@@AWS_ACCESS_KEY@@@@   \
                  --macro TENANT_INFO s3_access_key
appconfig autogen --replace /usr/lib/spark/spark-2.1.0-bin-hadoop2.7/conf/core-site.xml --pattern @@@@AWS_SECRET_KEY@@@@   \
                  --macro TENANT_INFO s3_secret_key
appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@SPARK_MASTER@@@@             \
                  --macro GET_SERVICE_URL spark_master controller

# node is expected to run the service.
#appconfig autogen --srvcid spark_master --sysv spark-master
#appconfig autogen --srvcid spark_worker --sysv spark-slave
#appconfig autogen --srvcid jupyter-notebook --sysv jupyter-server
#appconfig autogen --srvcid mysql --sysv mysqld
#appconfig autogen --srvcid zeppelin-notebook --sysv zeppelin-server

appconfig autogen --generate
appconfig package

## Specify a logo file for the
logo file --filepath Logo_Spark.png

################################################################################
#                        Catalog package for CentOS                            #
################################################################################
image build --basedir image/centos --imgversion 1.0 --os centos6
catalog save --filepath staging/jupyter100-centos.json --force
sources package
catalog package

################################################################################
#                        Catalog package for RHEL                              #
#                                                                              #
# Set RHEL_USERNAME and RHEL_PASSWORD environment variables before invoking    #
# the workbench. The Dockerfile uses subscription-manager to subscribe before  #
# installing any packages. The subscription is cleaned up at the end of the    #
# build process in the same Dockerfile.                                        #
################################################################################
#image build --basedir image/rhel --imgversion 1.0 --os rhel
#catalog save --filepath staging/spark200-rhel.json --force
#sources package
#catalog package --os=rhel
